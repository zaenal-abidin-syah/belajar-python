{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f0105c-1e9a-4f56-a282-fb00e2754b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a25961-8da0-4f36-96b7-e52cdf316be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all NLP related operations on text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882d9fcf-5db0-44cb-a49c-bf711bd423b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To mock web-browser and scrap tweets\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05a77fd1-bab2-41f5-86c5-97579c0420b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To consume Twitter's API\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a65f7054-e8bc-4515-8c4f-a9b8b1911ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in /home/zaens/anaconda3/envs/MachineLearning/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /home/zaens/anaconda3/envs/MachineLearning/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/zaens/anaconda3/envs/MachineLearning/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/zaens/anaconda3/envs/MachineLearning/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/zaens/anaconda3/envs/MachineLearning/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.66.4)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604c2725-3eef-4692-a382-2d1d3cecb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To identify the sentiment of text\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fbc4018-d0eb-4fc8-9dd8-9ce2dfcc32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring all the warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e51c3e-2b88-4a8c-a81c-48c8b04da933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zaens/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zaens/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/zaens/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/zaens/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/zaens/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/zaens/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to /home/zaens/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data] Downloading package brown to /home/zaens/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "# downloading stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('brown')\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7673c149-33d9-4846-9669-a0b992b9171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for showing all the plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc463d4c-166e-45c3-97a8-340c2a278629",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumClient(object):\n",
    "    def __init__(self):\n",
    "        #Initialization method. \n",
    "        self.chrome_options = webdriver.ChromeOptions()\n",
    "        self.chrome_options.add_argument('--headless')\n",
    "        self.chrome_options.add_argument('--no-sandbox')\n",
    "        self.chrome_options.add_argument('--disable-setuid-sandbox')\n",
    "\n",
    "        # you need to provide the path of chromdriver in your system\n",
    "        self.browser = webdriver.Chrome(options=self.chrome_options)\n",
    "\n",
    "        self.base_url = 'https://x.com/search?q='\n",
    "\n",
    "    def get_tweets(self, query):\n",
    "        #Function to fetch tweets. \n",
    "        try: \n",
    "            self.browser.get(self.base_url+query)\n",
    "            time.sleep(2)\n",
    "\n",
    "            body = self.browser.find_element_by_tag_name('body')\n",
    "\n",
    "            for _ in range(3000):\n",
    "                body.send_keys(Keys.PAGE_DOWN)\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            # timeline = self.browser.find_element_by_id('timeline')\n",
    "            # tweet_nodes = timeline.find_elements_by_css_selector('[data-testid=\"tweetText\"]')\n",
    "\n",
    "            tweet_nodes = self.browser.find_elements(By.CSS_SELECTOR, '[data-testid=\"tweetText\"]')\n",
    "            # time = self.browser.find_elements\n",
    "\n",
    "            return pd.DataFrame({'tweets': [tweet_node.text for tweet_node in tweet_nodes]})\n",
    "\n",
    "        \n",
    "        except:\n",
    "            print(\"Selenium - An error occured while fetching tweets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a584325c-45a0-4825-bb13-7c8cc5a5a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object): \n",
    "    def __init__(self): \n",
    "        #Initialization method. \n",
    "        try: \n",
    "            key = \"waFDOZSdeF6A0vZoiXFIJmVr6\"\n",
    "            secret = \"mOkLNM0BekQEduzj8Gn9RxXqZIa80oaoUJKweOxMRuktxmnNuZ\"\n",
    "            access_token = \"1492109235337175041-AAaO1RheWIGUsWBA7ecaoEUZLNymgG\"\n",
    "            access_token_secret = \"TchFoUVuuFQf3qKVFncVRxqK7X7ZZs4bcJ7jFILkbpJgh\"\n",
    "            # create OAuthHandler object \n",
    "            auth = OAuthHandler(key, secret) \n",
    "            # set access token and secret \n",
    "            auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http://172.22.218.218:8085'\"\n",
    "            self.api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "            # self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "            \n",
    "        except :\n",
    "        # except tweepy.TweepError as e:\n",
    "            print(f\"Error: Tweeter Authentication Failed - \")\n",
    "\n",
    "    def get_tweets(self, query, maxTweets = 1000):\n",
    "        #Function to fetch tweets. \n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "        sinceId = None\n",
    "        max_id = -1\n",
    "        tweetCount = 0\n",
    "        tweetsPerQry = 100\n",
    "\n",
    "        while tweetCount < maxTweets:\n",
    "            # try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets = self.api.search_tweets(q=query, count=tweetsPerQry)\n",
    "                else:\n",
    "                    new_tweets = self.api.search_tweets(q=query, count=tweetsPerQry,\n",
    "                                            since_id=sinceId)\n",
    "            else:\n",
    "                if (not sinceId):\n",
    "                    new_tweets = self.api.search_tweets(q=query, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets = self.api.search_tweets(q=query, count=tweetsPerQry,\n",
    "                                            max_id=str(max_id - 1),\n",
    "                                            since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print(\"No more tweets found\")\n",
    "                break\n",
    "\n",
    "            for tweet in new_tweets:\n",
    "                parsed_tweet = {} \n",
    "                parsed_tweet['tweets'] = tweet.text \n",
    "\n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "                    \n",
    "            tweetCount += len(new_tweets)\n",
    "            print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "            max_id = new_tweets[-1].id\n",
    "\n",
    "            # except :\n",
    "            # # except tweepy.TweepError as e:\n",
    "            #     # Just exit if any error\n",
    "            #     # print(\"Tweepy error : \" + str(e))\n",
    "            #     print(\"error\")\n",
    "            #     break\n",
    "        \n",
    "        return pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "238c3c1c-0d68-467f-a84f-1257e1d09bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium - An error occured while fetching tweets.\n",
      "tweets_df Shape - (0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selenium_client = SeleniumClient()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = selenium_client.get_tweets('AI and Deep learning')\n",
    "tweets = pd.DataFrame(tweets_df)\n",
    "print(f'tweets_df Shape - {tweets.shape}')\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c050b603-d906-4cdd-a1ae-a267f609bf2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m twitter_client \u001b[38;5;241m=\u001b[39m TwitterClient()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# calling function to get tweets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tweets_df \u001b[38;5;241m=\u001b[39m twitter_client\u001b[38;5;241m.\u001b[39mget_tweets(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAI and Deep learning\u001b[39m\u001b[38;5;124m'\u001b[39m, maxTweets\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweets_df Shape - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtweets_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m tweets_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[61], line 35\u001b[0m, in \u001b[0;36mTwitterClient.get_tweets\u001b[0;34m(self, query, maxTweets)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (max_id \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m sinceId):\n\u001b[0;32m---> 35\u001b[0m         new_tweets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39msearch_tweets(q\u001b[38;5;241m=\u001b[39mquery, count\u001b[38;5;241m=\u001b[39mtweetsPerQry)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         new_tweets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39msearch_tweets(q\u001b[38;5;241m=\u001b[39mquery, count\u001b[38;5;241m=\u001b[39mtweetsPerQry,\n\u001b[1;32m     38\u001b[0m                                 since_id\u001b[38;5;241m=\u001b[39msinceId)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/tweepy/api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/tweepy/api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_list\n\u001b[1;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/tweepy/api.py:1146\u001b[0m, in \u001b[0;36mAPI.search_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;129m@pagination\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;129m@payload\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_tweets\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"search_tweets(q, *, geocode, lang, locale, result_type, count, \\\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m                     until, since_id, max_id, include_entities)\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    .. _Twitter's documentation on the standard search API: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m   1147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch/tweets\u001b[39m\u001b[38;5;124m'\u001b[39m, endpoint_parameters\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeocode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1149\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muntil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msince_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_entities\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1150\u001b[0m         ), q\u001b[38;5;241m=\u001b[39mq, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1151\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/MachineLearning/lib/python3.11/site-packages/tweepy/api.py:271\u001b[0m, in \u001b[0;36mAPI.request\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(resp)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(resp)\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have access to a subset of Twitter API v2 endpoints and limited v1.1 endpoints (e.g. media post, oauth) only. If you need access to this endpoint, you may need a different access level. You can learn more here: https://developer.twitter.com/en/portal/product"
     ]
    }
   ],
   "source": [
    "twitter_client = TwitterClient()\n",
    "\n",
    "# calling function to get tweets\n",
    "tweets_df = twitter_client.get_tweets('AI and Deep learning', maxTweets=7000)\n",
    "print(f'tweets_df Shape - {tweets_df.shape}')\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce39f385-f570-43f3-84b6-10a0b81fd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweets(): \n",
    "    #Initialization method. \n",
    "    # try: \n",
    "    key = \"waFDOZSdeF6A0vZoiXFIJmVr6\"\n",
    "    secret = \"mOkLNM0BekQEduzj8Gn9RxXqZIa80oaoUJKweOxMRuktxmnNuZ\"\n",
    "    access_token = \"1492109235337175041-AAaO1RheWIGUsWBA7ecaoEUZLNymgG\"\n",
    "    access_token_secret = \"TchFoUVuuFQf3qKVFncVRxqK7X7ZZs4bcJ7jFILkbpJgh\"\n",
    "    # create OAuthHandler object \n",
    "    auth = OAuthHandler(key, secret) \n",
    "    print(auth)\n",
    "    # set access token and secret \n",
    "    auth.set_access_token(access_token, access_token_secret) \n",
    "    # create tweepy API object to fetch tweets \n",
    "    # add hyper parameter 'proxy' if executing from behind proxy \"proxy='http://172.22.218.218:8085'\"\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "        \n",
    "    # except :\n",
    "    # # except tweepy.TweepError as e:\n",
    "    #     print(f\"Error: Tweeter Authentication Failed - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb7b43a7-07f8-4916-a43f-539638331874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.auth.OAuthHandler object at 0x7f24a5221450>\n"
     ]
    }
   ],
   "source": [
    "tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b80c6f-9c90-4223-a200-de469613cd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf68d0b-c6e2-42ae-80a0-07de40e0db7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9c7fb-3184-454a-bd4b-ff5e853fef0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb135ed-0a89-4b7e-b327-4e1e36d3da04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce9dfd-ca66-4c44-93be-a42dda6262d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be243c3b-01f8-4c09-b326-3e3440afe27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110cbf1-b52e-401b-b59e-1f49bd6e1e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620421d2-c57b-4a88-b390-a591ebc18207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MachineLearning] *",
   "language": "python",
   "name": "conda-env-MachineLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
